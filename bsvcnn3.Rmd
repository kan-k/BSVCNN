---
title: "bsvcnn3"
output: html_document
---

Use a new (smaller) mask `/well/nichols/users/kfh142/data/Atlas/CIC/MultRes` namely `CICatlas_Res3_2mm.nii.gz`

```{r}
temp.img <-oro.nifti::readNIfTI('/well/nichols/users/kfh142/data/Atlas/CIC/MultRes/CICatlas_Res3_2mm.nii.gz')
length(unique(c(temp.img))) #number of region
```
Take `Res3` where `HO-combined-fin.nii.gz` is non-zero

```{r}
mask.com<-oro.nifti::readNIfTI('/well/nichols/users/qcv214/bsvcnn/pile/HO-combined-fin.nii.gz')

temp.img[mask.com==0] <- 0
temp.img@datatype = 2
temp.img@bitpix = 8
writeNIfTI(temp.img,paste0('/well/nichols/users/qcv214/bnn2/res3/res3mask'))
```
Then only keep GM using `CICatlas_Res3_trunc.txt`.


In fact let's not threshold by `HO-combined-fin.nii.gz`

`CICatlas_Res3_trunc.txt ` says everything above label 12 is not GM, then threshold at 0.1, so 
```{r}
temp.img <-oro.nifti::readNIfTI('/well/nichols/users/kfh142/data/Atlas/CIC/MultRes/CICatlas_Res3_2mm.nii.gz')
#Take only grey matter
temp.img[temp.img>12] <- 0
temp.img@datatype = 2
temp.img@bitpix = 8
writeNIfTI(temp.img,paste0('/well/nichols/users/qcv214/bnn2/res3/res3mask'))
#Load data with new unthreshold mask
part_list<-read.table('/well/nichols/users/qcv214/Placement_2/participant_list.txt', header = FALSE, sep = "", dec = ".") #4529 participants
part_list$exist_vbm <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_list[,1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz'))
part_use<-part_list[part_list$exist_vbm==1,] #4263 participants left
list_of_all_images<-paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_use[1:(dim(part_use)[1]),1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz')
res3.dat <- fast_read_imgs_mask(list_of_all_images,'/well/nichols/users/qcv214/bnn2/res3/res3mask') 
#threshold at 0.1
dat_colmeans<-colMeans(res3.dat)
temp.img[temp.img>1][dat_colmeans<0.1] <- 0 
temp.img@datatype = 2
temp.img@bitpix = 8
writeNIfTI(temp.img,'/well/nichols/users/qcv214/bnn2/res3/res3mask')
#Load data again using the new thresholded mask
res3.dat <- fast_read_imgs_mask(list_of_all_images,'/well/nichols/users/qcv214/bnn2/res3/res3mask') # 4263 x 163375
#save datat
write_feather(as.data.frame(res3.dat), '/well/nichols/users/qcv214/bnn2/res3/res3_dat.feather')
#reload data and mask
res3.dat <- read_feather('/well/nichols/users/qcv214/bnn2/res3/res3_dat.feather') #dim = 4263 124859
res3.mask <-oro.nifti::readNIfTI('/well/nichols/users/qcv214/bnn2/res3/res3mask.nii.gz')
res3.mask.reg <- setdiff(sort(unique(c(res3.mask))),0)
```

##Create mask ROI
Don't RUN
```{r}
#Don't RUN
res3.mask.reg <- sort(unique(c(res3.mask)))
for(i in res3.mask.reg){
  temp.img <-oro.nifti::readNIfTI('/well/nichols/users/qcv214/bnn2/res3/res3mask.nii.gz')
  temp.img <- temp.img == i 
  temp.img@datatype = 2
  temp.img@bitpix = 8
  writeNIfTI(temp.img,paste0('/well/nichols/users/qcv214/bnn2/res3/roi/mask_ROI_',i))
  temp.dat <- fast_read_imgs_mask(list_of_all_images,paste0('/well/nichols/users/qcv214/bnn2/res3/roi/mask_ROI_',i)) # 4263 x 163375
  write_feather(as.data.frame(temp.dat), paste0('/well/nichols/users/qcv214/bnn2/res3/roi/dat_ROI_',i))
}
```

Create `rearranged data`
```{r}
#dat.temp <- matrix(nrow = nrow(res3.dat),ncol = sum(res3.mask>0))
dat.temp <- matrix(nrow = nrow(res3.dat),ncol = 1)
  cum.length <- 0
for(i in res3.mask.reg){
  roi.reg<- read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/roi/dat_ROI_",i,'.feather'))
  print(dim(roi.reg))
  cum.length <- cum.length + ncol(roi.reg)
  print(cum.length)
  roi.reg<- as.matrix(roi.reg)
  dat.temp <- cbind(dat.temp, roi.reg)
}
colnames(dat.temp) <- NULL
dat.temp <- as.data.frame(dat.temp[,-1])
dat.temp <- as.matrix(dat.temp)
write_feather(as.data.frame(dat.temp), '/well/nichols/users/qcv214/bnn2/res3/dat_rearranged.feather')
```

Try loading partial gp from `partial_gp.R`
```{r}
  poly_degree = 10
  a_concentration = 0.5
  b_smoothness = 40
test.partial.gp <- read_feather("/well/nichols/users/qcv214/bnn2/res3/roi/partial_gp_1_fixed_100.540.feather")
```


#7 Mar
Save age to a file
```{r}
part_list<-read.table('/well/nichols/users/qcv214/Placement_2/participant_list.txt', header = FALSE, sep = "", dec = ".") #4529 participants
part_list$exist_vbm <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_list[,1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz'))
part_use<-part_list[part_list$exist_vbm==1,] #4263 participants left
# 
agetab<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Age.tsv', sep = '\t', header = TRUE)
age_tab<-as.data.frame(matrix(,nrow = length(part_use$V1),ncol = 2)) #id, age, number of masked voxels
colnames(age_tab)[1:2]<-c('id','age')
age_tab$id<-part_use$V1
for(i in 1:length(part_use$V1)){
  age_tab$age[i]<-agetab$X21003.2.0[agetab$eid_8107==sub(".", "",age_tab$id[i])]
}
write_feather(age_tab, '/well/nichols/users/qcv214/bnn2/res3/age.feather')
```

#8 Mar
Load data from `nn_lr.R`
```{r}
num.batch <- 4
epoch <- 10

lr_vec <- c(1,10^-(5:11)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,8,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_loss_",i,".csv")))
}
```
Plot training rmse
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))))
# lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 2)
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3)
# lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 4)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 5)
# lines(sqrt(res.nn.loss[1,6,]),type=line.type,col = 6)
lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 7)
# lines(sqrt(res.nn.loss[1,8,]),type=line.type,col = 8)

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))))
# lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 2)
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3)
# lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 4)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 5)
# lines(sqrt(res.nn.loss[2,6,]),type=line.type,col = 6)
lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 7)
# lines(sqrt(res.nn.loss[2,8,]),type=line.type,col = 8)
```



plot some
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:10,lwd=0.2)

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:10,lwd=0.2)

legend('bottomright',legend = c('learning_rate','1e-4','1e-6','1e-8','1e-10'), pch = 1,col=c("white",1,3,10,12))
```
14 Mar

Get the weights
```{r}
num.batch <- 4
epoch <- 10

#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_weights_",10^-10,".feather")))
```

```{r}
plot(density(w.1e10[1,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,])
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```




#21 Mar
Load data from `nn_lr3.R`
```{r}
num.batch <- 4
epoch <- 30

failed_run <- c(5,7,9,10)
lr_vec <- c(10^- setdiff((1:11),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,11,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn3_loss_",i,".csv")))
}

res.nn.loss <- res.nn.loss[, 1:length(lr_vec),]

```
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:10,lwd=0.2)

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:10,lwd=0.2)

legend('bottomright',legend = c('learning_rate','1e-01','1e-3','1e-6','1e-8'), pch = 1,col=c("white",1,3,10,12))
```
Get the weights
```{r}
num.batch <- 4
epoch <- 30
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn3_weights_",10^-1,".feather")))
```

```{r}
plot(density(w.1e10[1,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,])
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```


#22 Mar
Load data from `nn_lr4.R`
```{r}
num.batch <- 4
epoch <- 30

failed_run <- c(4,5,9)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn4_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),]
```
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate','0.1','0.3','0.7'), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate','0.1','0.3','0.7'), pch = 1,col=c("white",1,3,10))
```
Get theta
```{r}
num.batch <- 4
epoch <- 30
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn4_theta_",0.3,".feather")))
plot(density(w.1e10[1,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```


Load data from `nn_lr5.R`

40 iterations in 2hrs and 20 mins
```{r}
num.batch <- 4
epoch <- 40

failed_run <- c(5,9)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn5_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),]
```
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate','0.2','0.4','0.7'), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate','0.1','0.3','0.6'), pch = 1,col=c("white",1,3,10))
```
Get theta
```{r}
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn5_theta_",0.6,".feather")))
plot(density(w.1e10[1,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```



#23 Mar
##Newton method
Load data from `nn_lr_nm1.R`
```{r}
num.batch <- 4
epoch <- 30
failed_run<-c()
res.nn.loss <- array(, dim = c(2,3,num.batch*epoch )) 
for( i in setdiff(2:4,failed_run)){  
  res.nn.loss[,which(i==2:4),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_nm1_loss_jobid_",i,".csv")))
}

```
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 4,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('run','1','2'), pch = 1,col=c("white",1,3))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 4,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('run','1','2'), pch = 1,col=c("white",1,3))

plot(sqrt(res.nn.loss[2,1,]),type=line.type,lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 4,lwd=2)
legend('bottomleft',legend = c('run','1','2'), pch = 1,col=c("white",1,3))


plot(sqrt(res.nn.loss[2,3,]),type=line.type,lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,1,]),type=line.type,col = 4,lwd=2)
legend('bottomleft',legend = c('run','1','2'), pch = 1,col=c("white",1,3))

```
It's not working.. and green line is kinda unpredictable 










#24 Mar
Load whole brain for benchmark from `whole_brain.R`
poly_degree = 10
a_concentration = 0.5
b_smoothness = 40
```{r}
poly_degree = 10
a_concentration = 0.5
b_smoothness = 40
wb_23mar<-array(,dim=c(50,3,14))
failed_run<-c()
for(i in setdiff(1:50,failed_run)){
      wb_23mar[i,,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/whole_brain",poly_degree,a_concentration,b_smoothness,"_",i,".csv")))
}
#bsvcnn.11feb.def<-bsvcnn.11feb.def[-failed_run,,]
wb_23mar.m<-apply(wb_23mar, c(2,3), median)
plot.box(wb_23mar)
wb_23mar.m[1,3]
```

```{r}
plot.box(wb_23mar)
```


#25 Mar and 27 mar
I think batch size too small 
##Adam
Load data from `nn_adam.R`
```{r}
num.batch <- 20
epoch <- 50

failed_run <- c()
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){
  # print(i)
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam1_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```

```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('topleft',legend = c('learning_rate',as.character(lr_vec[c(1,3,5,7)])), pch = 1,col=c("white",1,3,10,12))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,3,5,7)])), pch = 1,col=c("white",1,3,10,12))
```
Get theta
```{r}
num.batch <- 4
epoch <- 20
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam1_theta_",0.1,".feather")))
plot(density(w.1e10[4,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```

##Adam with de-coupled weights
Load data from `nn_adamW.R`

```{r}
num.batch <- 20
epoch <- 50

failed_run <- c(4,5)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW1_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),]
```

```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate','0.1','0.2','0.3'), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate','0.1','0.2','0.3'), pch = 1,col=c("white",1,3,10))
```
Get theta
```{r}
num.batch <- 4
epoch <- 10
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW1_theta_",0.1,".feather")))
plot(density(w.1e10[1,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```


#28 Mar
##Adam
Load data from `nn_adam.R`
```{r}
num.batch <- 4
epoch <- 20

failed_run <- c(4,5,6,7)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){
   print(i)
  print(which(i==lr_vec))
  if(i == 1){res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",i,".csv")))}
  else{res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam2_loss_",i,".csv")))[,1:80]}
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```
```{r}
num.batch <- 4
epoch <- 20

failed_run <- c(4,5,6,7)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
res.nn.loss[,1,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",0.1,".csv")))
for( i in lr_vec){
   print(i)
  print(which(i==lr_vec))
res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam2_loss_",i,".csv")))[,1:80]
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```

```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,3,5)])), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,3,5)])), pch = 1,col=c("white",1,3,10))

plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")

res.nn.loss2 <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",0.1,".csv")))
plot(sqrt(res.nn.loss2[2,]),type=line.type, ylim = c(min(sqrt(res.nn.loss2[2,])), max(sqrt(res.nn.loss2[2,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
```
Get theta
```{r}
num.batch <- 4
epoch <- 20
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_theta_",0.1,".feather")))
plot(density(w.1e10[7,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```


##AdamW
Load data from `nn_adamW.R`
```{r}
num.batch <- 4
epoch <- 30

failed_run <- c(3,5,7,8,9)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){
   print(i)
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW2_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,3,5)])), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2,4)])), pch = 1,col=c("white",1,3,10))
```
Get theta
```{r}
num.batch <- 4
epoch <- 30
#Get the best performance
w.1e10 <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW2_theta_",0.1,".feather")))
plot(density(w.1e10[4,]))
# lines(density(w.1e10[2,]))
# lines(density(w.1e10[3,]))
for(i in 1:12){
  qqnorm(w.1e10[i,], main = paste0("Normal Q-Q Plot of Region #",i))
  #abline(a=0,b=1, col = 'red')
  qqline(w.1e10[i,])
}
```


##Summmary so far
In the setting of 2,000 training, 2,000 testing with 500 batch size and 20 epochs
1.SGD with lr = 0.6 (originally had 40 epochs) `nn_lr5.R`
2.newton method jobid 2
3. Adam with lr = 0.1, too many hyperparameters to set 
4. Adam with weight decay with lr = 0.3
5. Adam with small batch (not working so well), lr = 0.1
6. [Benchmark] whole brain 
###Load all data 
```{r}
#SGD
res.nn.loss.sgd <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn5_loss_",0.6,".csv")))
#Newton
res.nn.loss.nm <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_nm1_loss_jobid_",4,".csv")))
#Adam
res.nn.loss.adam <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",0.1,".csv")))
#Adam with decoupled weight decay
res.nn.loss.adamw <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW2_loss_",0.3,".csv")))
#Adam with small batch
res.nn.loss.adam_s <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam1_loss_",0.1,".csv")))
```
###Load whole brain
```{r}
poly_degree = 10
a_concentration = 0.5
b_smoothness = 40
wb_23mar<-array(,dim=c(50,3,14))
failed_run<-c()
for(i in setdiff(1:50,failed_run)){
      wb_23mar[i,,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/whole_brain",poly_degree,a_concentration,b_smoothness,"_",i,".csv")))
}
wb_23mar.m<-apply(wb_23mar, c(2,3), median)
```

###Concat the results
```{r}
min.it <- min(ncol(res.nn.loss.sgd),ncol(res.nn.loss.nm),ncol(res.nn.loss.adam),ncol(res.nn.loss.adamw),ncol(res.nn.loss.adam_s))
full.res.out <- rbind(sqrt(res.nn.loss.sgd[2,1:min.it]),sqrt(res.nn.loss.nm[2,1:min.it]), sqrt(res.nn.loss.adam[2,1:min.it]), sqrt(res.nn.loss.adamw[2,1:min.it]), sqrt(res.nn.loss.adam_s[2,1:min.it]),wb_23mar.m[1,6])
full.res.in <- rbind(sqrt(res.nn.loss.sgd[1,1:min.it]),sqrt(res.nn.loss.nm[1,1:min.it]), sqrt(res.nn.loss.adam[1,1:min.it]), sqrt(res.nn.loss.adamw[1,1:min.it]), sqrt(res.nn.loss.adam_s[1,1:min.it]),wb_23mar.m[1,5])

full.res <- array(,dim=c(2,dim(full.res.in)))
full.res[1,,] <- full.res.in
full.res[2,,] <- full.res.out
```

```{r}
line.type <- 'l'
plot(full.res[1,1,],type=line.type, ylim = c(min(full.res[1,,]), max(full.res[1,,])),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(full.res[1,2,],type=line.type,col = 2,lwd=2)
lines(full.res[1,3,],type=line.type,col = 3,lwd=2)
lines(full.res[1,4,],type=line.type,col = 4,lwd=2)
lines(full.res[1,5,],type=line.type,col = 5,lwd=2)
lines(full.res[1,6,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Newton","Adam", "AdamW", "Adam_s","Whole"), pch = 1,col=c("white",1:6))

#out-of-sample
plot(full.res[2,1,],type=line.type, ylim = c(min(full.res[2,,]), 
                                             7#max(full.res[2,,])
                                             ),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(full.res[2,2,],type=line.type,col = 2,lwd=2)
lines(full.res[2,3,],type=line.type,col = 3,lwd=2)
lines(full.res[2,4,],type=line.type,col = 4,lwd=2)
lines(full.res[2,5,],type=line.type,col = 5,lwd=2)
lines(full.res[2,6,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Newton","Adam", "AdamW", "Adam_s","Whole"), pch = 1,col=c("white",1:6))
```


Deleting Newton and Adam_s

```{r}
min.it <- min(ncol(res.nn.loss.sgd),ncol(res.nn.loss.adam),ncol(res.nn.loss.adamw),ncol(res.nn.loss.adam_s))
full.res.out <- rbind(sqrt(res.nn.loss.sgd[2,1:min.it]), sqrt(res.nn.loss.adam[2,1:min.it]), sqrt(res.nn.loss.adamw[2,1:min.it]),wb_23mar.m[1,6])
full.res.in <- rbind(sqrt(res.nn.loss.sgd[1,1:min.it]), sqrt(res.nn.loss.adam[1,1:min.it]), sqrt(res.nn.loss.adamw[1,1:min.it]),wb_23mar.m[1,5])

full.res <- array(,dim=c(2,dim(full.res.in)))
full.res[1,,] <- full.res.in
full.res[2,,] <- full.res.out
```

```{r}
line.type <- 'l'
plot(full.res[1,1,], type = "o", lty = 1,pch=1, ylim = c(min(full.res[1,-c(2,5),]), max(full.res[1,-c(2,5),])),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
# lines(full.res[1,2,],type=line.type,col = 2,lwd=2)
lines(full.res[1,2,], type = "o", lty = 2,pch=2,col = 3,lwd=2)
lines(full.res[1,3,], type = "o", lty = 3,pch=3,col = 4,lwd=2)
# lines(full.res[1,5,],type=line.type,col = 5,lwd=2)
lines(full.res[1,4,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Adam", "AdamW","WB_Reg"),lty=c(1,1,2,3,1), pch = c(1,1,2,3,20),col=c("white",1,3,4,6))

#out-of-sample
plot(full.res[2,1,], type = "o", lty = 1,pch=1, ylim = c(min(full.res[2,-c(2,5),]), max(full.res[2,-c(2,5),])),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
# lines(full.res[2,2,],type=line.type,col = 2,lwd=2)
lines(full.res[2,2,], type = "o", lty = 2,pch=2,col = 3,lwd=2)
lines(full.res[2,3,], type = "o", lty = 3,pch=3,col = 4,lwd=2)
# lines(full.res[2,5,],type=line.type,col = 5,lwd=2)
lines(full.res[2,4,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Adam", "AdamW","WB_Reg"),lty=c(1,1,2,3,1), pch = c(1,1,2,3,20),col=c("white",1,3,4,6))
```

Tom's meeting 28 March

1. deffo fix bias for each regression
2. Look into the scattered mask
3.Investigate loading time
Maybe
4. Try to mimic the neural nets to look similar to whole brain GP

Investigating mask
`CICatlas_Res3_trunc.txt ` says everything above label 12 is not GM, then threshold at 0.1, so 
```{r}
temp.img <-oro.nifti::readNIfTI('/well/nichols/users/kfh142/data/Atlas/CIC/MultRes/CICatlas_Res3_2mm.nii.gz')
#Take only grey matter
temp.img[temp.img>12] <- 0
temp.img@datatype = 2
temp.img@bitpix = 8
writeNIfTI(temp.img,paste0('/well/nichols/users/qcv214/bnn2/res3/maskfix/res3mask_1'))
#Load data with new unthreshold mask
part_list<-read.table('/well/nichols/users/qcv214/Placement_2/participant_list.txt', header = FALSE, sep = "", dec = ".") #4529 participants
part_list$exist_vbm <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_list[,1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz'))
part_use<-part_list[part_list$exist_vbm==1,] #4263 participants left
list_of_all_images<-paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_use[1:(dim(part_use)[1]),1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz')
res3.dat <- fast_read_imgs_mask(list_of_all_images,'/well/nichols/users/qcv214/bnn2/res3/maskfix/res3mask_1') 
#threshold at 0.1
dat_colmeans<-colMeans(res3.dat)
#Originally I had temp.img > 1, that means it's not thresholded by 
# temp.img[temp.img>1][dat_colmeans<0.1] <- 0 
# temp.img@datatype = 2
# temp.img@bitpix = 8
# writeNIfTI(temp.img,'/well/nichols/users/qcv214/bnn2/res3/maskfix/res3mask_2')

#
temp.img[temp.img>0][dat_colmeans<0.1] <- 0 
temp.img@datatype = 2
temp.img@bitpix = 8
writeNIfTI(temp.img,'/well/nichols/users/qcv214/bnn2/res3/maskfix/res3mask_3') #Correct
writeNIfTI(temp.img,'/well/nichols/users/qcv214/bnn2/res3/res3mask')

#Load data again using the new thresholded mask
res3.dat <- fast_read_imgs_mask(list_of_all_images,'/well/nichols/users/qcv214/bnn2/res3/res3mask') # 4263 x 156812
#save datat
write_feather(as.data.frame(res3.dat), '/well/nichols/users/qcv214/bnn2/res3/res3_dat.feather')
#reload data and mask
res3.dat <- read_feather('/well/nichols/users/qcv214/bnn2/res3/res3_dat.feather') #dim = 4263 156812
res3.mask <-oro.nifti::readNIfTI('/well/nichols/users/qcv214/bnn2/res3/res3mask.nii.gz')
res3.mask.reg <- setdiff(sort(unique(c(res3.mask))),0)
```
Create `rearranged data`
```{r}
#dat.temp <- matrix(nrow = nrow(res3.dat),ncol = sum(res3.mask>0))
dat.temp <- matrix(nrow = nrow(res3.dat),ncol = 1)
  cum.length <- 0
for(i in res3.mask.reg){
  roi.reg<- read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/roi/dat_ROI_",i,'.feather'))
  print(dim(roi.reg))
  cum.length <- cum.length + ncol(roi.reg)
  print(cum.length)
  roi.reg<- as.matrix(roi.reg)
  dat.temp <- cbind(dat.temp, roi.reg)
}
colnames(dat.temp) <- NULL
dat.temp <- as.data.frame(dat.temp[,-1])
dat.temp <- as.matrix(dat.temp)
write_feather(as.data.frame(dat.temp), '/well/nichols/users/qcv214/bnn2/res3/dat_rearranged.feather')
```


Load data from `nn_lr6.R`
```{r}
num.batch <- 4
epoch <-30

failed_run <- c(3,5,7,8,9)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){  
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn6_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),]
```
```{r}
line.type <- 'b'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,3,4)])), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 10,lwd=2)
#lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,3,4)])), pch = 1,col=c("white",1,3,10))
```

##Summmary so far
In the setting of 2,000 training, 2,000 testing with 500 batch size and 20 epochs
1.SGD with lr = 0.6 (originally had 40 epochs) `nn_lr5.R`
2.newton method jobid 2
3. Adam with lr = 0.1, too many hyperparameters to set 
4. Adam with weight decay with lr = 0.3
5. Adam with small batch (not working so well), lr = 0.1
6. [Benchmark] whole brain 
###Load all data 
```{r}
#SGD
res.nn.loss.sgd <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn6_loss_",0.6,".csv")))
#Newton
res.nn.loss.nm <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_nm1_loss_jobid_",4,".csv")))
#Adam
res.nn.loss.adam <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",0.1,".csv")))
#Adam with decoupled weight decay
res.nn.loss.adamw <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adamW2_loss_",0.1,".csv")))
#Adam with small batch
res.nn.loss.adam_s <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam1_loss_",0.1,".csv")))
```
###Load whole brain
```{r}
poly_degree = 10
a_concentration = 0.5
b_smoothness = 40
wb_23mar<-array(,dim=c(50,3,14))
failed_run<-c()
for(i in setdiff(1:50,failed_run)){
      wb_23mar[i,,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/whole_brain",poly_degree,a_concentration,b_smoothness,"_",i,".csv")))
}
wb_23mar.m<-apply(wb_23mar, c(2,3), median)
```

###Concat the results
```{r}
min.it <- min(ncol(res.nn.loss.sgd),ncol(res.nn.loss.nm),ncol(res.nn.loss.adam),ncol(res.nn.loss.adamw),ncol(res.nn.loss.adam_s))
full.res.out <- rbind(sqrt(res.nn.loss.sgd[2,1:min.it]),sqrt(res.nn.loss.nm[2,1:min.it]), sqrt(res.nn.loss.adam[2,1:min.it]), sqrt(res.nn.loss.adamw[2,1:min.it]), sqrt(res.nn.loss.adam_s[2,1:min.it]),wb_23mar.m[1,6])
full.res.in <- rbind(sqrt(res.nn.loss.sgd[1,1:min.it]),sqrt(res.nn.loss.nm[1,1:min.it]), sqrt(res.nn.loss.adam[1,1:min.it]), sqrt(res.nn.loss.adamw[1,1:min.it]), sqrt(res.nn.loss.adam_s[1,1:min.it]),wb_23mar.m[1,5])

full.res <- array(,dim=c(2,dim(full.res.in)))
full.res[1,,] <- full.res.in
full.res[2,,] <- full.res.out
```

```{r}
line.type <- 'l'
plot(full.res[1,1,],type=line.type, ylim = c(min(full.res[1,,]), max(full.res[1,,])),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(full.res[1,2,],type=line.type,col = 2,lwd=2)
lines(full.res[1,3,],type=line.type,col = 3,lwd=2)
lines(full.res[1,4,],type=line.type,col = 4,lwd=2)
lines(full.res[1,5,],type=line.type,col = 5,lwd=2)
lines(full.res[1,6,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Newton","Adam", "AdamW", "Adam_s","Whole"), pch = 1,col=c("white",1:6))

#out-of-sample
plot(full.res[2,1,],type=line.type, ylim = c(min(full.res[2,,]), min(max(full.res[2,,]),7)),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(full.res[2,2,],type=line.type,col = 2,lwd=2)
lines(full.res[2,3,],type=line.type,col = 3,lwd=2)
lines(full.res[2,4,],type=line.type,col = 4,lwd=2)
lines(full.res[2,5,],type=line.type,col = 5,lwd=2)
lines(full.res[2,6,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomright',legend = c('method',"SGD","Newton","Adam", "AdamW", "Adam_s","Whole"), pch = 1,col=c("white",1:6))
```

Deleting Newton and Adam_s

```{r}
min.it <- min(ncol(res.nn.loss.sgd),ncol(res.nn.loss.adam),ncol(res.nn.loss.adamw),ncol(res.nn.loss.adam_s))
full.res.out <- rbind(sqrt(res.nn.loss.sgd[2,1:min.it]), sqrt(res.nn.loss.adam[2,1:min.it]), sqrt(res.nn.loss.adamw[2,1:min.it]),wb_23mar.m[1,6])
full.res.in <- rbind(sqrt(res.nn.loss.sgd[1,1:min.it]), sqrt(res.nn.loss.adam[1,1:min.it]), sqrt(res.nn.loss.adamw[1,1:min.it]),wb_23mar.m[1,5])

full.res <- array(,dim=c(2,dim(full.res.in)))
full.res[1,,] <- full.res.in
full.res[2,,] <- full.res.out
```

```{r}
line.type <- 'l'
plot(full.res[1,1,], type = "o", lty = 1,pch=1, ylim = c(min(full.res[1,,]), max(full.res[1,,])),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
# lines(full.res[1,2,],type=line.type,col = 2,lwd=2)
lines(full.res[1,2,], type = "o", lty = 2,pch=2,col = 3,lwd=2)
lines(full.res[1,3,], type = "o", lty = 3,pch=3,col = 4,lwd=2)
# lines(full.res[1,5,],type=line.type,col = 5,lwd=2)
lines(full.res[1,4,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('method',"SGD","Adam", "AdamW","WB_Reg"),lty=c(1,1,2,3,1), pch = c(1,1,2,3,20),col=c("white",1,3,4,6))

#out-of-sample
plot(full.res[2,1,], type = "o", lty = 1,pch=1, ylim = c(min(full.res[2,,]), max(full.res[2,,])),lwd=2,
     main = "Validation RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
# lines(full.res[2,2,],type=line.type,col = 2,lwd=2)
lines(full.res[2,2,], type = "o", lty = 2,pch=2,col = 3,lwd=2)
lines(full.res[2,3,], type = "o", lty = 3,pch=3,col = 4,lwd=2)
# lines(full.res[2,5,],type=line.type,col = 5,lwd=2)
lines(full.res[2,4,],type=line.type,col = 6,lwd=2)
#abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('method',"SGD","Adam", "AdamW","WB_Reg"),lty=c(1,1,2,3,1), pch = c(1,1,2,3,20),col=c("white",1,3,4,6))
```

```{r}
plot(sqrt(res.nn.loss.adam[2,1:min.it]))
```




#After adding bias
##Adam
Load data from `nn_adam_b1.R`
```{r}
num.batch <- 4
epoch <- 20

failed_run <- c(1,3,6,8)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb_adam1_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,3,5)])), pch = 1,col=c("white",1,3,10))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,3,5)])), pch = 1,col=c("white",1,3,10))
```

Get bias
```{r}
num.batch <- 4
epoch <- 20
#Get the best performance
w.1e10 <- as.matrix(read.csv('/well/nichols/users/qcv214/bnn2/res3/pile/nnb_adam1_theta_0.2.csv'))
plot(w.1e10)
```

##SGD
Load data from `nnb1.R`
Not working great
```{r}
num.batch <- 4
epoch <- 20

failed_run <- c(1:6,9)
lr_vec <- c(10^(-1)*setdiff((1:9),failed_run)) #Note that 1 is not lr = 1, but lr = 1e-4
res.nn.loss <- array(, dim = c(2,9,num.batch*epoch )) 
for( i in lr_vec){
  res.nn.loss[,which(i==lr_vec),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb1_loss_",i,".csv")))
}
res.nn.loss <- res.nn.loss[, 1:length(lr_vec),] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
# lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
# lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```


##Comparison of Adam with and without bias
```{r}
num.batch <- 4
epoch <- 20
res.nn.loss <- array(, dim = c(2,2,num.batch*epoch )) 
res.nn.loss[,1,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nn_adam3_loss_",0.1,".csv")))
res.nn.loss[,2,] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb_adam1_loss_",0.2,".csv")))
```

```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
legend('topright',legend = c('method','w/o bias','w bias'), pch = 1,col=c("white",1,3))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Validation RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)

abline(v=4*1:epoch,lwd=0.2)

legend('topright',legend = c('method','w/o bias','w bias'), pch = 1,col=c("white",1,3))
```


#30 mar
- Look at time taken to load data on Rstudio vs R (ivybridge).... it only takes 1.77 mintues to load on Rstudio

- Visualise the GP of each region (actually probably don't need that) but lets try it anyway, and maybe the ReLU??


*** I am starting t osuspect that my way of inputting data has been wrong, I have been inputting re-arranged data as oppposed to raw data. But I believe that my GP copes with raw data

##Check the result from `viz_gp_test.R`

plotting partial GP (first term of basis)
```{r}
bases.nb <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/bnn2/res3/roi/partial_gp_",2,"_fixed_",poly_degree,a_concentration,b_smoothness,".feather")))
gp.mask.hs <- mask_subcor
gp.mask.hs[gp.mask.hs!=0] <-bases.nb[1,]
gp.mask.hs@datatype = 16
gp.mask.hs@bitpix = 32
writeNIfTI(gp.mask.hs,paste0('/well/nichols/users/qcv214/bnn2/res3/viz/Reg2_partial_gp'))
```

##Check the test result from `nn_dat_test.R`
This is for re-arranged data
```{r}
num.batch <- 4
epoch <- 10
failed_run <- c(1,3,4)
res.nn.loss.t1 <- array(, dim = c(2,5,num.batch*epoch )) 
for( i in setdiff(1:5,failed_run)){
  res.nn.loss.t1[,i,] <-as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/test_re-arranged_nnb1_loss_0.7_jobid_",i,".csv")))
}
res.nn.loss.t1 <- res.nn.loss.t1[,-failed_run,] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss.t1[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss.t1[1,,])), max(sqrt(res.nn.loss.t1[1,,])))
     ,lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
 lines(sqrt(res.nn.loss.t1[1,2,]),type=line.type,col = 3,lwd=2)
# lines(sqrt(res.nn.loss.t1[1,3,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss.t1[1,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
# legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss.t1[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss.t1[2,,])), max(sqrt(res.nn.loss.t1[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
 lines(sqrt(res.nn.loss.t1[2,2,]),type=line.type,col = 3,lwd=2)
#  lines(sqrt(res.nn.loss.t1[2,3,]),type=line.type,col = 10,lwd=2)
# lines(sqrt(res.nn.loss.t1[2,7,]),type=line.type,col = 12,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

# legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```


This is for raw data
```{r}
num.batch <- 4
epoch <- 10

failed_run <- c()
res.nn.loss.t2 <- array(, dim = c(2,5,num.batch*epoch )) 
for( i in setdiff(1:5,failed_run)){
  res.nn.loss.t2[,i,] <-as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/test_raw_nnb1_loss_0.7_jobid_",i+5,".csv")))
}
 # res.nn.loss.t2 <- res.nn.loss.t2[,-failed_run,] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss.t2[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss.t2[1,,])), max(sqrt(res.nn.loss.t2[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss.t2[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss.t2[1,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss.t2[1,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss.t2[1,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
# legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss.t2[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss.t2[2,,])), max(sqrt(res.nn.loss.t2[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss.t2[2,2,]),type=line.type,col = 3,lwd=2)
 lines(sqrt(res.nn.loss.t2[2,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss.t2[2,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss.t2[2,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

# legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```




#31 Mar
After fixing data

##SGD
Load data from `nnb2.R`
```{r}
learning_rate <- 0.6
num.batch <- 4
epoch <- 40

failed_run <- c()
res.nn.loss <- array(, dim = c(2,10,num.batch*epoch )) 
for( i in setdiff(1:10,failed_run)){
  res.nn.loss[,i,] <-as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb2_loss_",learning_rate,"_jobid_",i,".csv")))
}
 # res.nn.loss <- res.nn.loss[,-failed_run,] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
# legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
 lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

# legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```

##Adam
Load data from `nn_adamb2.R`
```{r}
learning_rate <- 0.1
num.batch <- 4
epoch <- 40

failed_run <- c()
res.nn.loss <- array(, dim = c(2,10,num.batch*epoch )) 
for( i in setdiff(1:10,failed_run)){
  res.nn.loss[,i,] <-as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb_adam2_loss_",learning_rate,"_jobid_",i,".csv")))
}
 # res.nn.loss <- res.nn.loss[,-failed_run,] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
# legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
 lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

# legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```

##SGD
Load data from `nnb2.R`
```{r}
learning_rate <- 0.1
num.batch <- 4
epoch <- 40

failed_run <- c()
res.nn.loss <- array(, dim = c(2,10,num.batch*epoch )) 
for( i in setdiff(1:10,failed_run)){
  res.nn.loss[,i,] <-as.matrix(read.csv(paste0("/well/nichols/users/qcv214/bnn2/res3/pile/nnb_adamW2_loss_",learning_rate,"_jobid_",i,".csv")))
}
 # res.nn.loss <- res.nn.loss[,-failed_run,] #set range
```
```{r}
line.type <- 'l'
plot(sqrt(res.nn.loss[1,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[1,,])), max(sqrt(res.nn.loss[1,,]))),lwd=2,
     main = "Training RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[1,2,]),type=line.type,col = 3,lwd=2)
lines(sqrt(res.nn.loss[1,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[1,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[1,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)
# legend('bottomleft',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",12))

#out-of-sample
plot(sqrt(res.nn.loss[2,1,]),type=line.type, ylim = c(min(sqrt(res.nn.loss[2,,])), max(sqrt(res.nn.loss[2,,]))),lwd=2,
     main = "Out-of-sample RMSE",
     ylab = "Batch RMSE",xlab= "Iteration")
lines(sqrt(res.nn.loss[2,2,]),type=line.type,col = 3,lwd=2)
 lines(sqrt(res.nn.loss[2,3,]),type=line.type,col = 10,lwd=2)
lines(sqrt(res.nn.loss[2,4,]),type=line.type,col = 12,lwd=2)
lines(sqrt(res.nn.loss[2,5,]),type=line.type,col = 14,lwd=2)
abline(v=4*1:epoch,lwd=0.2)

# legend('topright',legend = c('learning_rate',as.character(lr_vec[c(1,2)])), pch = 1,col=c("white",1,2))
```




